{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf898f9-8c9f-4e1d-9239-7199df96ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "from torch.utils.data import DataLoader, Dataset # Ensure Dataset is imported\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "DATASET_PATH = r\"C:\\Users\\priya\\OneDrive\\Desktop\\Student Attentiveness dataset\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# Data transforms (NO data augmentation)\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        # Add your desired data augmentation here for training\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# --- Robust Data Loading with separate transforms for subsets ---\n",
    "class DatasetWithTransform(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper to apply different transforms to subsets created by random_split.\n",
    "    \"\"\"\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get item from the subset's original dataset using the subset's internal indexing\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "# --- Data Loading ---\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset path not found: {DATASET_PATH}\")\n",
    "\n",
    "train_dataset_full = datasets.ImageFolder(os.path.join(DATASET_PATH)) # No transform here initially\n",
    "# The transform will be applied by the wrapper DatasetWithTransform later\n",
    "\n",
    "val_split = 0.2\n",
    "train_size = int((1 - val_split) * len(train_dataset_full))\n",
    "val_size = len(train_dataset_full) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset_full, [train_size, val_size])\n",
    "\n",
    "# Now wrap the subsets with their specific transforms\n",
    "train_dataset = DatasetWithTransform(train_subset, data_transforms['train'])\n",
    "val_dataset = DatasetWithTransform(val_subset, data_transforms['val'])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "# --- Load pretrained EfficientNet B4 Model ---\n",
    "weights = EfficientNet_B4_Weights.DEFAULT\n",
    "model = efficientnet_b4(weights=weights)\n",
    "\n",
    "# Freeze feature extractor layers\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.classifier[-1].in_features\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss() # Use BCELoss with Sigmoid output\n",
    "# Optimize only the classifier parameters as feature extractor is frozen\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        images, labels = images.to(DEVICE), labels.float().to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.round()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    all_val_probs = [] # Raw probabilities before rounding\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            images, labels = images.to(DEVICE), labels.float().to(DEVICE).unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs.round()\n",
    "\n",
    "            all_val_preds.append(preds.cpu().numpy())\n",
    "            all_val_labels.append(labels.cpu().numpy())\n",
    "            all_val_probs.append(outputs.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Concatenate all collected results after the loop for epoch-level metrics\n",
    "    all_val_preds = np.concatenate(all_val_preds).flatten()\n",
    "    all_val_labels = np.concatenate(all_val_labels).flatten()\n",
    "    all_val_probs = np.concatenate(all_val_probs).flatten()\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    val_acc = np.mean(all_val_preds == all_val_labels)\n",
    "    val_precision = precision_score(all_val_labels, all_val_preds, zero_division=0) # Added zero_division\n",
    "    val_recall = recall_score(all_val_labels, all_val_preds, zero_division=0)     # Added zero_division\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, zero_division=0)             # Added zero_division\n",
    "\n",
    "    try:\n",
    "        val_roc_auc = roc_auc_score(all_val_labels, all_val_probs)\n",
    "    except ValueError:\n",
    "        val_roc_auc = float('nan') # Not a Number\n",
    "        print(\"Warning: ROC AUC cannot be calculated (only one class present in validation labels).\")\n",
    "\n",
    "\n",
    "    print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1-Score: {val_f1:.4f}, Val ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"eye_attention_model_efficientnet_b4_no_aug.pth\")\n",
    "print(\"Training complete. Model saved as eye_attention_model_efficientnet_b4_no_aug.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a0258-b3f8-4c64-a385-845a5fb75582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
    "import os\n",
    "\n",
    "# --- Configuration (MUST MATCH TRAINING CONFIG) ---\n",
    "IMG_SIZE = 224\n",
    "MODEL_PATH = \"eye_attention_model_efficientnet_b4_no_aug.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 1 # Binary classification output\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Model Definition (MUST MATCH TRAINING MODEL ARCHITECTURE) ---\n",
    "def create_efficientnet_b4_model(num_classes=1, pretrained=True, freeze_features=False, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Re-defines the EfficientNet B4 model exactly as it was defined during training.\n",
    "    Important: `freeze_features` should be False if you loaded a fully fine-tuned model.\n",
    "    `dropout_rate` should match the best dropout rate found during training.\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        weights = EfficientNet_B4_Weights.DEFAULT\n",
    "        model = efficientnet_b4(weights=weights)\n",
    "    else:\n",
    "        model = efficientnet_b4()\n",
    "\n",
    "    # Freeze features if the model was trained in a two-stage process where features were frozen initially\n",
    "    # For inference, if you loaded a fully fine-tuned model, you don't need to freeze.\n",
    "    # If you only trained the head, then you would freeze.\n",
    "    if freeze_features: # This should typically be False for loading a fully trained model\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    num_ftrs = model.classifier[-1].in_features\n",
    "\n",
    "    # Recreate the exact same custom classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(dropout_rate), # Use the dropout rate from training\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(dropout_rate), # Use the dropout rate from training\n",
    "        nn.Linear(4096, num_classes),\n",
    "        nn.Sigmoid() # Sigmoid is part of the model output for BCELoss\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# --- Load the Trained Model ---\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Trained model not found at: {MODEL_PATH}. Please ensure it's in the correct path.\")\n",
    "\n",
    "model = create_efficientnet_b4_model(num_classes=NUM_CLASSES, pretrained=True, freeze_features=False, dropout_rate=0.5) # Set freeze_features to False if loading a fully fine-tuned model\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "print(f\"Model loaded from {MODEL_PATH} and set to evaluation mode.\")\n",
    "\n",
    "# --- Preprocessing Transforms for Inference (MUST MATCH Validation Transforms) ---\n",
    "preprocess_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(), # Convert OpenCV BGR image (numpy array) to PIL Image\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Webcam Capture and Inference Loop ---\n",
    "def run_webcam_inference():\n",
    "    cap = cv2.VideoCapture(0) # 0 for default webcam\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    print(\"Webcam opened. Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read() # Read a frame from the webcam\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame from BGR (OpenCV default) to RGB (PyTorch/PIL expects RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Preprocess the frame for the model\n",
    "        input_tensor = preprocess_transforms(frame_rgb).unsqueeze(0).to(DEVICE) # Add batch dimension\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probability = torch.sigmoid(output).item() # Get the probability\n",
    "            prediction = 1 if probability >= 0.5 else 0 # Threshold to get class label\n",
    "\n",
    "        # Display prediction on the frame\n",
    "        label_text = \"Attentive\" if prediction == 1 else \"Not Attentive\"\n",
    "        color = (0, 255, 0) if prediction == 1 else (0, 0, 255) # Green for attentive, Red for not attentive\n",
    "\n",
    "        cv2.putText(frame, f\"Status: {label_text} ({probability:.2f})\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Gazenet Attention Detector', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release() # Release the webcam\n",
    "    cv2.destroyAllWindows() # Close all OpenCV windows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_webcam_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e9bce-96c4-44c6-b1ad-d371c0ac4b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
